{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Novel Cascade Binary Tagging Framework for Relational Triple Extraction ACL 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1前言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1,1课程回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/overall_for_code.png' width=\"800\" height=\"800\" align=\"bottom\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 模型结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/casrel.png\"  width=\"600\" height=\"600\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 代码结构展示\n",
    "<img src=\"./imgs/directory.jpg\"  width=\"300\" height=\"300\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 准备工作\n",
    "### 2.1项目环境配置\n",
    "\n",
    "* Python3.8\n",
    "* jupyter notebook\n",
    "* torch            1.6.0+cu10.2\n",
    "* numpy            1.18.5\n",
    "* transformers       3.4.0\n",
    "\n",
    "代码运行环境建议使用Visual Studio Code(VScode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 数据集下载\n",
    "NYT数据集下载地址：https://drive.google.com/file/d/10f24s9gM7NdyO3z5OqQxJgYud4NnCJg3/view <br> \n",
    "WEBNLG数据集下载地址：https://drive.google.com/file/d/1zISxYa-8ROe2Zv8iRc82jY9QsQrfY1Vj/view <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 项目代码结构（VScode中演示）\n",
    "\n",
    ">1）是什么？\n",
    "\n",
    "　　我们首先会在VScode环境中让代码跑一下，直观感受到项目的训练，并展示前向推断的输出，让大家看到模型的效果。\n",
    ">2）怎么构成的？\n",
    "\n",
    "　　然后介绍项目代码的构成，介绍项目有哪些文件夹，包含哪些文件，这些文件构成了什么功能模块如：数据预处理模块，模型设计模块，损失函数模块，推断与评估模块。\n",
    ">3）小结\n",
    "\n",
    "　　在主文件中在过一下启动训练的流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 算法模块及细节（jupyter和VScode中演示）\n",
    "\n",
    "　　在jupyter notebook中细致地讲解每一个模块。\n",
    "  \n",
    "　　以实现模块功能为目的，来讲解每个函数的执行流程，呈现中间数据，方便同学们理解学习。\n",
    "  \n",
    "　　内容分为以下几个模块：**超参数设置，数据读取与处理，模型定义，模型训练，模型评价**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "import config\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_name', type=str, default='Casrel', help='name of the model')\n",
    "parser.add_argument('--lr', type=float, default=1e-5)\n",
    "parser.add_argument('--multi_gpu', type=bool, default=False)\n",
    "parser.add_argument('--dataset', type=str, default='NYT')\n",
    "parser.add_argument('--batch_size', type=int, default=6)\n",
    "parser.add_argument('--max_epoch', type=int, default=15) #300\n",
    "parser.add_argument('--test_epoch', type=int, default=1)\n",
    "parser.add_argument('--train_prefix', type=str, default='train_triples')\n",
    "parser.add_argument('--dev_prefix', type=str, default='dev_triples')\n",
    "parser.add_argument('--test_prefix', type=str, default='test_triples')\n",
    "parser.add_argument('--max_len', type=int, default=150)\n",
    "parser.add_argument('--rel_num', type=int, default=25)\n",
    "parser.add_argument('--period', type=int, default=50)\n",
    "parser.add_argument('--debug', type=bool, default=False)\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "con = config.Config(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_config(con):\n",
    "    for key in con.__dict__:\n",
    "        print(key, end=' = ')\n",
    "        print(con.__dict__[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args = Namespace(batch_size=6, dataset='NYT', debug=False, dev_prefix='dev_triples', lr=1e-05, max_epoch=15, max_len=150, model_name='Casrel', multi_gpu=False, period=50, rel_num=25, test_epoch=1, test_prefix='test_triples', train_prefix='train_triples')\n",
      "multi_gpu = False\n",
      "learning_rate = 1e-05\n",
      "batch_size = 6\n",
      "max_epoch = 15\n",
      "max_len = 150\n",
      "rel_num = 25\n",
      "dataset = NYT\n",
      "root = /home/niuhao/project/RE/CasRel_2020\n",
      "data_path = /home/niuhao/project/RE/CasRel_2020/data/NYT\n",
      "checkpoint_dir = /home/niuhao/project/RE/CasRel_2020/checkpoint/NYT\n",
      "log_dir = /home/niuhao/project/RE/CasRel_2020/log/NYT\n",
      "result_dir = /home/niuhao/project/RE/CasRel_2020/result/NYT\n",
      "train_prefix = train_triples\n",
      "dev_prefix = dev_triples\n",
      "test_prefix = test_triples\n",
      "model_save_name = Casrel_DATASET_NYT_LR_1e-05_BS_6\n",
      "log_save_name = LOG_Casrel_DATASET_NYT_LR_1e-05_BS_6\n",
      "result_save_name = RESULT_Casrel_DATASET_NYT_LR_1e-05_BS_6.json\n",
      "period = 50\n",
      "test_epoch = 1\n",
      "debug = False\n"
     ]
    }
   ],
   "source": [
    "print_config(con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 数据读取与处理\n",
    "* 数据处理细节\n",
    "* 构建dataset类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 数据处理细节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open(os.path.join(con.data_path, con.train_prefix + '.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2id = json.load(open(os.path.join(con.data_path, 'rel2id.json')))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56195"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Massachusetts ASTON MAGNA Great Barrington ; also at Bard College , Annandale-on-Hudson , N.Y. , July 1-Aug .',\n",
       " 'triple_list': [['Annandale-on-Hudson',\n",
       "   '/location/location/contains',\n",
       "   'College']]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'North Carolina EASTERN MUSIC FESTIVAL Greensboro , June 25-July 30 .'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = train_data[1]['text']\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'North Carolina EASTERN MUSIC FESTIVAL Greensboro , June 25-July 30 .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(text.split())\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['north',\n",
       " 'carolina',\n",
       " 'eastern',\n",
       " 'music',\n",
       " 'festival',\n",
       " 'greensboro',\n",
       " ',',\n",
       " 'june',\n",
       " '25',\n",
       " '-',\n",
       " 'july',\n",
       " '30',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2ro_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Weiner', '/people/person/place_lived', 'Queens'],\n",
       " ['Weiner', '/people/person/place_lived', 'Brooklyn']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[6]['triple_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Carolina', '/location/location/contains', 'Greensboro']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple = train_data[1]['triple_list'][0]\n",
    "triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['carolina']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(triple[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['carolina'], '/location/location/contains', ['greensboro'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple = (tokenizer.tokenize(triple[0]), triple[1], tokenizer.tokenize(triple[2]))\n",
    "triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "source = tokens\n",
    "target = triple[0]\n",
    "target_len = len(target)\n",
    "for i in range(len(source)):\n",
    "    if source[i: i + target_len] == target:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_head_idx(source, target):\n",
    "    target_len = len(target)\n",
    "    for i in range(len(source)):\n",
    "        if source[i: i + target_len] == target:\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_head_idx = find_head_idx(tokens, triple[0])\n",
    "print(sub_head_idx)\n",
    "obj_head_idx = find_head_idx(tokens, triple[2])\n",
    "obj_head_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = (sub_head_idx, sub_head_idx + len(triple[0]) - 1)\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 1): []}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if sub not in s2ro_map:\n",
    "    s2ro_map[sub] = []\n",
    "s2ro_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 1): [(5, 5, 22)]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2ro_map[sub].append((obj_head_idx, obj_head_idx + len(triple[2]) - 1, rel2id[triple[1]]))\n",
    "s2ro_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2167, 3792, 2789, 2189, 2782, 27905, 1010, 2238, 2423, 1011, 2251, 2382, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = tokenizer(text)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = out['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  2167,  3792,  2789,  2189,  2782, 27905,  1010,  2238,\n",
       "        2423,  1011,  2251,  2382,  1012,   102])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = np.array(token_ids)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_len = len(tokens)\n",
    "text_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_heads, sub_tails = np.zeros(text_len), np.zeros(text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 1): [(5, 5, 22)]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2ro_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [s for s in s2ro_map][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_heads[s[0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tails[s[1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_heads, obj_tails = np.zeros((text_len, con.rel_num)), np.zeros((text_len, con.rel_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.rel_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 25)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_head_idx, sub_tail_idx = choice(list(s2ro_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_head_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ro in s2ro_map.get((sub_head_idx, sub_tail_idx), []):\n",
    "    obj_heads[ro[0]][ro[2]] = 1\n",
    "    obj_tails[ro[1]][ro[2]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_tails[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 将上述功能构建为类\n",
    "* 将相似类型的功能整合到一个类里，使代码结构更清晰，并且方面以后调佣，且有利于debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMEDDataset(Dataset):\n",
    "    def __init__(self, config, prefix, is_test, tokenizer):\n",
    "        self.config = config\n",
    "        self.prefix = prefix\n",
    "        self.is_test = is_test\n",
    "        self.tokenizer = tokenizer\n",
    "        if self.config.debug:\n",
    "            self.json_data = pickle.load(open(os.path.join(self.config.data_path, prefix + '.pkl'), 'rb'))[:500]\n",
    "        else:\n",
    "            self.json_data = pickle.load(open(os.path.join(self.config.data_path, prefix + '.pkl'), 'rb'))\n",
    "        self.rel2id = json.load(open(os.path.join(self.config.data_path, 'rel2id.json')))[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.json_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ins_json_data = self.json_data[idx]\n",
    "        text = ins_json_data['text']\n",
    "        text = ' '.join(text.split()[:self.config.max_len])\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        if len(tokens) > BERT_MAX_LEN:\n",
    "            tokens = tokens[: BERT_MAX_LEN]\n",
    "        text_len = len(tokens)\n",
    "\n",
    "        if not self.is_test:\n",
    "            s2ro_map = {}\n",
    "            for triple in ins_json_data['triple_list']:\n",
    "                triple = (self.tokenizer.tokenize(triple[0]), triple[1], self.tokenizer.tokenize(triple[2]))\n",
    "                sub_head_idx = find_head_idx(tokens, triple[0])\n",
    "                obj_head_idx = find_head_idx(tokens, triple[2])\n",
    "                if sub_head_idx != -1 and obj_head_idx != -1:\n",
    "                    sub = (sub_head_idx, sub_head_idx + len(triple[0]) - 1)\n",
    "                    if sub not in s2ro_map:\n",
    "                        s2ro_map[sub] = []\n",
    "                    s2ro_map[sub].append((obj_head_idx, obj_head_idx + len(triple[2]) - 1, self.rel2id[triple[1]]))\n",
    "\n",
    "            if s2ro_map:\n",
    "                # token_ids, segment_ids = self.tokenizer.encode(first=text)\n",
    "                tokenizer_out = self.tokenizer(text)\n",
    "                token_ids = tokenizer_out['input_ids']\n",
    "                segment_ids = tokenizer_out['attention_mask']\n",
    "                masks = segment_ids\n",
    "                if len(token_ids) > text_len:\n",
    "                    token_ids = token_ids[:text_len]\n",
    "                    masks = masks[:text_len]\n",
    "                token_ids = np.array(token_ids)\n",
    "                masks = np.array(masks) + 1\n",
    "                sub_heads, sub_tails = np.zeros(text_len), np.zeros(text_len)\n",
    "                for s in s2ro_map:\n",
    "                    sub_heads[s[0]] = 1\n",
    "                    sub_tails[s[1]] = 1\n",
    "                sub_head_idx, sub_tail_idx = choice(list(s2ro_map.keys()))\n",
    "                sub_head, sub_tail = np.zeros(text_len), np.zeros(text_len)\n",
    "                sub_head[sub_head_idx] = 1\n",
    "                sub_tail[sub_tail_idx] = 1\n",
    "                obj_heads, obj_tails = np.zeros((text_len, self.config.rel_num)), np.zeros((text_len, self.config.rel_num))\n",
    "                for ro in s2ro_map.get((sub_head_idx, sub_tail_idx), []):\n",
    "                    obj_heads[ro[0]][ro[2]] = 1\n",
    "                    obj_tails[ro[1]][ro[2]] = 1\n",
    "                return token_ids, masks, text_len, sub_heads, sub_tails, sub_head, sub_tail, obj_heads, obj_tails, ins_json_data['triple_list'], tokens\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            # token_ids, segment_ids = self.tokenizer.encode(first=text)\n",
    "            tokenizer_out = self.tokenizer(text)\n",
    "            token_ids = tokenizer_out['input_ids']\n",
    "            segment_ids = tokenizer_out['attention_mask']\n",
    "            masks = segment_ids\n",
    "            if len(token_ids) > text_len:\n",
    "                token_ids = token_ids[:text_len]\n",
    "                masks = masks[:text_len]\n",
    "            token_ids = np.array(token_ids)\n",
    "            masks = np.array(masks) + 1\n",
    "            sub_heads, sub_tails = np.zeros(text_len), np.zeros(text_len)\n",
    "            sub_head, sub_tail = np.zeros(text_len), np.zeros(text_len)\n",
    "            obj_heads, obj_tails = np.zeros((text_len, self.config.rel_num)), np.zeros((text_len, self.config.rel_num))\n",
    "            return token_ids, masks, text_len, sub_heads, sub_tails, sub_head, sub_tail, obj_heads, obj_tails, ins_json_data['triple_list'], tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmed_collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    batch.sort(key=lambda x: x[2], reverse=True)\n",
    "    token_ids, masks, text_len, sub_heads, sub_tails, sub_head, sub_tail, obj_heads, obj_tails, triples, tokens = zip(*batch)\n",
    "    cur_batch = len(batch)\n",
    "    max_text_len = max(text_len)\n",
    "    batch_token_ids = torch.LongTensor(cur_batch, max_text_len).zero_()\n",
    "    batch_masks = torch.LongTensor(cur_batch, max_text_len).zero_()\n",
    "    batch_sub_heads = torch.Tensor(cur_batch, max_text_len).zero_()\n",
    "    batch_sub_tails = torch.Tensor(cur_batch, max_text_len).zero_()\n",
    "    batch_sub_head = torch.Tensor(cur_batch, max_text_len).zero_()\n",
    "    batch_sub_tail = torch.Tensor(cur_batch, max_text_len).zero_()\n",
    "    batch_obj_heads = torch.Tensor(cur_batch, max_text_len, 44).zero_()\n",
    "    batch_obj_tails = torch.Tensor(cur_batch, max_text_len, 44).zero_()\n",
    "\n",
    "    for i in range(cur_batch):\n",
    "        batch_token_ids[i, :text_len[i]].copy_(torch.from_numpy(token_ids[i]))\n",
    "        batch_masks[i, :text_len[i]].copy_(torch.from_numpy(masks[i]))\n",
    "        batch_sub_heads[i, :text_len[i]].copy_(torch.from_numpy(sub_heads[i]))\n",
    "        batch_sub_tails[i, :text_len[i]].copy_(torch.from_numpy(sub_tails[i]))\n",
    "        batch_sub_head[i, :text_len[i]].copy_(torch.from_numpy(sub_head[i]))\n",
    "        batch_sub_tail[i, :text_len[i]].copy_(torch.from_numpy(sub_tail[i]))\n",
    "        batch_obj_heads[i, :text_len[i], :].copy_(torch.from_numpy(obj_heads[i]))\n",
    "        batch_obj_tails[i, :text_len[i], :].copy_(torch.from_numpy(obj_tails[i]))\n",
    "\n",
    "    return {'token_ids': batch_token_ids,\n",
    "            'mask': batch_masks,\n",
    "            'sub_heads': batch_sub_heads,\n",
    "            'sub_tails': batch_sub_tails,\n",
    "            'sub_head': batch_sub_head,\n",
    "            'sub_tail': batch_sub_tail,\n",
    "            'obj_heads': batch_obj_heads,\n",
    "            'obj_tails': batch_obj_tails,\n",
    "            'triples': triples,\n",
    "            'tokens': tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(config, prefix, is_test=False, num_workers=0, collate_fn=cmed_collate_fn):\n",
    "    dataset = CMEDDataset(config, prefix, is_test, tokenizer)\n",
    "    if not is_test:\n",
    "        data_loader = DataLoader(dataset=dataset,\n",
    "                                 batch_size=config.batch_size,\n",
    "                                 shuffle=True,\n",
    "                                 pin_memory=True,\n",
    "                                 num_workers=num_workers,\n",
    "                                 collate_fn=collate_fn)\n",
    "    else:\n",
    "        data_loader = DataLoader(dataset=dataset,\n",
    "                                 batch_size=1,\n",
    "                                 shuffle=False,\n",
    "                                 pin_memory=True,\n",
    "                                 num_workers=num_workers,\n",
    "                                 collate_fn=collate_fn)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreFetcher(object):\n",
    "    def __init__(self, loader):\n",
    "        self.loader = iter(loader)\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.preload()\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_data = next(self.loader)\n",
    "        except StopIteration:\n",
    "            self.next_data = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            for k, v in self.next_data.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    self.next_data[k] = self.next_data[k].cuda(non_blocking=True)\n",
    "\n",
    "    def next(self):\n",
    "        torch.cuda.current_stream().wait_stream(self.stream)\n",
    "        data = self.next_data\n",
    "        self.preload()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.0'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import data_loader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Casrel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Casrel, self).__init__()\n",
    "        self.config = config\n",
    "        self.bert_dim = 768\n",
    "        self.bert_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.sub_heads_linear = nn.Linear(self.bert_dim, 1)\n",
    "        self.sub_tails_linear = nn.Linear(self.bert_dim, 1)\n",
    "        self.obj_heads_linear = nn.Linear(self.bert_dim, self.config.rel_num)\n",
    "        self.obj_tails_linear = nn.Linear(self.bert_dim, self.config.rel_num)\n",
    "\n",
    "    def get_objs_for_specific_sub(self, sub_head_mapping, sub_tail_mapping, encoded_text):\n",
    "        # [batch_size, 1, bert_dim]\n",
    "        sub_head = torch.matmul(sub_head_mapping, encoded_text)\n",
    "        # [batch_size, 1, bert_dim]\n",
    "        sub_tail = torch.matmul(sub_tail_mapping, encoded_text)\n",
    "        # [batch_size, 1, bert_dim]\n",
    "        sub = (sub_head + sub_tail) / 2\n",
    "        # [batch_size, seq_len, bert_dim]\n",
    "        encoded_text = encoded_text + sub\n",
    "        # [batch_size, seq_len, rel_num]\n",
    "        pred_obj_heads = self.obj_heads_linear(encoded_text)\n",
    "        pred_obj_heads = torch.sigmoid(pred_obj_heads)\n",
    "        # [batch_size, seq_len, rel_num]\n",
    "        pred_obj_tails = self.obj_tails_linear(encoded_text)\n",
    "        pred_obj_tails = torch.sigmoid(pred_obj_tails)\n",
    "        return pred_obj_heads, pred_obj_tails\n",
    "\n",
    "    def get_encoded_text(self, token_ids, mask):\n",
    "        # [batch_size, seq_len, bert_dim(768)]\n",
    "        encoded_text = self.bert_encoder(token_ids, attention_mask=mask)[0]\n",
    "        return encoded_text\n",
    "\n",
    "    def get_subs(self, encoded_text):\n",
    "        # [batch_size, seq_len, 1]\n",
    "        pred_sub_heads = self.sub_heads_linear(encoded_text)\n",
    "        pred_sub_heads = torch.sigmoid(pred_sub_heads)\n",
    "        # [batch_size, seq_len, 1]\n",
    "        pred_sub_tails = self.sub_tails_linear(encoded_text)\n",
    "        pred_sub_tails = torch.sigmoid(pred_sub_tails)\n",
    "        return pred_sub_heads, pred_sub_tails\n",
    "\n",
    "    def forward(self, data):\n",
    "        # [batch_size, seq_len]\n",
    "        token_ids = data['token_ids']\n",
    "        # [batch_size, seq_len]\n",
    "        mask = data['mask']\n",
    "        # [batch_size, seq_len, bert_dim(768)]\n",
    "        encoded_text = self.get_encoded_text(token_ids, mask)\n",
    "        # [batch_size, seq_len, 1]\n",
    "        pred_sub_heads, pred_sub_tails = self.get_subs(encoded_text)\n",
    "        # [batch_size, 1, seq_len]\n",
    "        sub_head_mapping = data['sub_head'].unsqueeze(1)\n",
    "        # [batch_size, 1, seq_len]\n",
    "        sub_tail_mapping = data['sub_tail'].unsqueeze(1)\n",
    "        # [batch_size, seq_len, rel_num]\n",
    "        pred_obj_heads, pred_obj_tails = self.get_objs_for_specific_sub(sub_head_mapping, sub_tail_mapping, encoded_text)\n",
    "        return pred_sub_heads, pred_sub_tails, pred_obj_heads, pred_obj_tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Casrel(\n",
       "  (bert_encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (sub_heads_linear): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (sub_tails_linear): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (obj_heads_linear): Linear(in_features=768, out_features=25, bias=True)\n",
       "  (obj_tails_linear): Linear(in_features=768, out_features=25, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_model = Casrel(con)\n",
    "ori_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, ori_model.parameters()), lr=con.learning_rate)\n",
    "\n",
    "# whether use multi GPU\n",
    "if con.multi_gpu:\n",
    "    model = nn.DataParallel(ori_model)\n",
    "else:\n",
    "    model = ori_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "def loss(gold, pred, mask):\n",
    "    pred = pred.squeeze(-1)\n",
    "    los = F.binary_cross_entropy(pred, gold, reduction='none')\n",
    "    if los.shape != mask.shape:\n",
    "        mask = mask.unsqueeze(-1)\n",
    "    los = torch.sum(los * mask) / torch.sum(mask)\n",
    "    return los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the checkpoint dir\n",
    "if not os.path.exists(con.checkpoint_dir):\n",
    "    os.mkdir(con.checkpoint_dir)\n",
    "\n",
    "# check the log dir\n",
    "if not os.path.exists(con.log_dir):\n",
    "    os.mkdir(con.log_dir)\n",
    "\n",
    "# get the data loader\n",
    "train_data_loader = data_loader.get_loader(con, prefix=con.train_prefix)\n",
    "dev_data_loader = data_loader.get_loader(con, prefix=con.dev_prefix, is_test=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "global_step = 0\n",
    "loss_sum = 0\n",
    "\n",
    "best_f1_score = 0\n",
    "best_precision = 0\n",
    "best_recall = 0\n",
    "\n",
    "best_epoch = 0\n",
    "init_time = time.time()\n",
    "start_time = time.time()\n",
    "\n",
    "# the training loop\n",
    "# for epoch in range(self.config.max_epoch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_prefetcher = data_loader.DataPreFetcher(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_ids': tensor([[  101,  2096,  1996, 28709,  6790,  2001, 25369,  2041,  2005,  2049,\n",
       "           1005,  1005, 15085,  7720,  8158,  1998,  3824,  6904, 18796,  3215,\n",
       "           1010,  1005,  1005,  2429,  2000,  1996, 25022, 12380,  2015,  4773,\n",
       "           2609,  1010, 24526,  1005,  1055,  4825,  1010,  1999,  2225,  2121,\n",
       "           3077,  1010,  4058,  1010,  2363,  1996,  2087,  3784,  4494,  1997,\n",
       "           2274, 13527],\n",
       "         [  101, 20351,  2546,  1010,  2321,  1010,  2040,  2973,  2006,  2358,\n",
       "           1012,  6017,  3927,  1999,  1996,  3134,  3077,  2930,  1997,  6613,\n",
       "           1010,  2351,  2012, 26756,  2902,  2415,  2044,  2108, 13263,  1999,\n",
       "           1996, 13878,  2076,  1037,  2954,  1999,  1996,  4257,  9711,  2078,\n",
       "           2212,  2008,  2920,  2195, 12908,  1010,  1996,  2610,  2056,     0,\n",
       "              0,     0],\n",
       "         [  101,  6744,  1005,  1055,  3570,  2003,  2028,  1997,  1996,  2087,\n",
       "           7591,  3980,  1999,  1996,  5611,  1011,  9302,  4736,  1010,  2007,\n",
       "           3956,  6815,  2035,  1997,  1996,  2103,  2004,  2049,  3007,  1010,\n",
       "           2096,  1996, 21524,  6148,  1037,  2925,  3007,  1999,  1996,  2103,\n",
       "           1005,  1055,  2789,  4753,  1012,  1005,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  4171,  5651,  2013,  1996,  3054,  1011,  3150,  1005,  1055,\n",
       "           3662,  2008,  2720,  1012, 10967,  5809,  2018,  2499,  2005,  1996,\n",
       "           8050, 13246,  1998, 11970,  3840,  1999,  3481,  3389,  1010,  1050,\n",
       "           1012,  1046,  1012,  1010,  1998,  2005,  3576, 11970,  4297,  1012,\n",
       "           1010,  2036,  1999,  2047,  3933,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  1999,  2014,  2197,  2622,  1010,  1005,  1005,  4175,  8189,\n",
       "           5897,  1010,  1005,  1005,  3491,  2012,  6254,  2050,  2340,  1999,\n",
       "          27884,  1010,  2762,  1010,  1999,  2526,  1010,  5796,  1012,  9092,\n",
       "           3591,  3263,  6361,  9668,  1997,  3901,  1997,  4068,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  3539,  2704, 22070,  4013,  4305,  1997,  3304,  2038,  2036,\n",
       "           2587,  1996,  5981,  1012,  1005,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]], device='cuda:0'),\n",
       " 'mask': tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2],\n",
       "         [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 0, 0, 0],\n",
       "         [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0,\n",
       "          0, 0, 0, 0],\n",
       "         [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0,\n",
       "          0, 0, 0, 0],\n",
       "         [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0],\n",
       "         [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0]], device='cuda:0'),\n",
       " 'sub_heads': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "        device='cuda:0'),\n",
       " 'sub_tails': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "        device='cuda:0'),\n",
       " 'sub_head': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "        device='cuda:0'),\n",
       " 'sub_tail': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "        device='cuda:0'),\n",
       " 'obj_heads': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 1., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " 'obj_tails': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 1., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " 'triples': [[['Ohio', '/location/location/contains', 'Westerville']],\n",
       "  [['Brooklyn', '/location/location/contains', 'Weeksville'],\n",
       "   ['Weeksville', '/location/neighborhood/neighborhood_of', 'Brooklyn']],\n",
       "  [['Israel', '/location/country/capital', 'Jerusalem'],\n",
       "   ['Israel', '/location/location/contains', 'Jerusalem']],\n",
       "  [['Jersey', '/location/location/contains', 'Lee']],\n",
       "  [['Germany', '/location/country/administrative_divisions', 'Berlin'],\n",
       "   ['Berlin', '/location/administrative_division/country', 'Germany'],\n",
       "   ['Germany', '/location/country/capital', 'Berlin'],\n",
       "   ['Germany', '/location/location/contains', 'Berlin'],\n",
       "   ['Germany', '/location/location/contains', 'Kassel']],\n",
       "  [['Prodi', '/people/person/nationality', 'Italy']]],\n",
       " 'tokens': [['while',\n",
       "   'the',\n",
       "   'borg',\n",
       "   '##ata',\n",
       "   'was',\n",
       "   'singled',\n",
       "   'out',\n",
       "   'for',\n",
       "   'its',\n",
       "   \"'\",\n",
       "   \"'\",\n",
       "   'inviting',\n",
       "   'marble',\n",
       "   'floors',\n",
       "   'and',\n",
       "   'contemporary',\n",
       "   'fa',\n",
       "   '##uce',\n",
       "   '##ts',\n",
       "   ',',\n",
       "   \"'\",\n",
       "   \"'\",\n",
       "   'according',\n",
       "   'to',\n",
       "   'the',\n",
       "   'ci',\n",
       "   '##nta',\n",
       "   '##s',\n",
       "   'web',\n",
       "   'site',\n",
       "   ',',\n",
       "   'wendell',\n",
       "   \"'\",\n",
       "   's',\n",
       "   'restaurant',\n",
       "   ',',\n",
       "   'in',\n",
       "   'west',\n",
       "   '##er',\n",
       "   '##ville',\n",
       "   ',',\n",
       "   'ohio',\n",
       "   ',',\n",
       "   'received',\n",
       "   'the',\n",
       "   'most',\n",
       "   'online',\n",
       "   'votes',\n",
       "   'of',\n",
       "   'five',\n",
       "   'finalists',\n",
       "   '.'],\n",
       "  ['sharif',\n",
       "   '##f',\n",
       "   ',',\n",
       "   '15',\n",
       "   ',',\n",
       "   'who',\n",
       "   'lived',\n",
       "   'on',\n",
       "   'st',\n",
       "   '.',\n",
       "   'marks',\n",
       "   'avenue',\n",
       "   'in',\n",
       "   'the',\n",
       "   'weeks',\n",
       "   '##ville',\n",
       "   'section',\n",
       "   'of',\n",
       "   'brooklyn',\n",
       "   ',',\n",
       "   'died',\n",
       "   'at',\n",
       "   'bellevue',\n",
       "   'hospital',\n",
       "   'center',\n",
       "   'after',\n",
       "   'being',\n",
       "   'stabbed',\n",
       "   'in',\n",
       "   'the',\n",
       "   'abdomen',\n",
       "   'during',\n",
       "   'a',\n",
       "   'fight',\n",
       "   'in',\n",
       "   'the',\n",
       "   'flat',\n",
       "   '##iro',\n",
       "   '##n',\n",
       "   'district',\n",
       "   'that',\n",
       "   'involved',\n",
       "   'several',\n",
       "   'teenagers',\n",
       "   ',',\n",
       "   'the',\n",
       "   'police',\n",
       "   'said',\n",
       "   '.'],\n",
       "  ['jerusalem',\n",
       "   \"'\",\n",
       "   's',\n",
       "   'status',\n",
       "   'is',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'most',\n",
       "   'sensitive',\n",
       "   'questions',\n",
       "   'in',\n",
       "   'the',\n",
       "   'israeli',\n",
       "   '-',\n",
       "   'palestinian',\n",
       "   'conflict',\n",
       "   ',',\n",
       "   'with',\n",
       "   'israel',\n",
       "   'claiming',\n",
       "   'all',\n",
       "   'of',\n",
       "   'the',\n",
       "   'city',\n",
       "   'as',\n",
       "   'its',\n",
       "   'capital',\n",
       "   ',',\n",
       "   'while',\n",
       "   'the',\n",
       "   'palestinians',\n",
       "   'seek',\n",
       "   'a',\n",
       "   'future',\n",
       "   'capital',\n",
       "   'in',\n",
       "   'the',\n",
       "   'city',\n",
       "   \"'\",\n",
       "   's',\n",
       "   'eastern',\n",
       "   'sector',\n",
       "   '.',\n",
       "   \"'\",\n",
       "   \"'\"],\n",
       "  ['tax',\n",
       "   'returns',\n",
       "   'from',\n",
       "   'the',\n",
       "   'mid',\n",
       "   '-',\n",
       "   '1980',\n",
       "   \"'\",\n",
       "   's',\n",
       "   'showed',\n",
       "   'that',\n",
       "   'mr',\n",
       "   '.',\n",
       "   'vic',\n",
       "   '##tus',\n",
       "   'had',\n",
       "   'worked',\n",
       "   'for',\n",
       "   'the',\n",
       "   'fundamental',\n",
       "   'minerals',\n",
       "   'and',\n",
       "   'metals',\n",
       "   'corporation',\n",
       "   'in',\n",
       "   'fort',\n",
       "   'lee',\n",
       "   ',',\n",
       "   'n',\n",
       "   '.',\n",
       "   'j',\n",
       "   '.',\n",
       "   ',',\n",
       "   'and',\n",
       "   'for',\n",
       "   'minor',\n",
       "   'metals',\n",
       "   'inc',\n",
       "   '.',\n",
       "   ',',\n",
       "   'also',\n",
       "   'in',\n",
       "   'new',\n",
       "   'jersey',\n",
       "   '.'],\n",
       "  ['in',\n",
       "   'her',\n",
       "   'last',\n",
       "   'project',\n",
       "   ',',\n",
       "   \"'\",\n",
       "   \"'\",\n",
       "   'count',\n",
       "   '##ena',\n",
       "   '##nce',\n",
       "   ',',\n",
       "   \"'\",\n",
       "   \"'\",\n",
       "   'shown',\n",
       "   'at',\n",
       "   'document',\n",
       "   '##a',\n",
       "   '11',\n",
       "   'in',\n",
       "   'kassel',\n",
       "   ',',\n",
       "   'germany',\n",
       "   ',',\n",
       "   'in',\n",
       "   '2002',\n",
       "   ',',\n",
       "   'ms',\n",
       "   '.',\n",
       "   'tan',\n",
       "   'presented',\n",
       "   '200',\n",
       "   'filmed',\n",
       "   'portraits',\n",
       "   'of',\n",
       "   'residents',\n",
       "   'of',\n",
       "   'berlin',\n",
       "   '.'],\n",
       "  ['prime',\n",
       "   'minister',\n",
       "   'romano',\n",
       "   'pro',\n",
       "   '##di',\n",
       "   'of',\n",
       "   'italy',\n",
       "   'has',\n",
       "   'also',\n",
       "   'joined',\n",
       "   'the',\n",
       "   'debate',\n",
       "   '.',\n",
       "   \"'\",\n",
       "   \"'\"]]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = train_data_prefetcher.next()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sub_heads, pred_sub_tails, pred_obj_heads, pred_obj_tails = model(data)\n",
    "# print('pred_sub_heads:{}, pred_sub_tails: {},pred_obj_heads: {},pred_obj_tails: {}'.format(pred_sub_heads, pred_sub_tails, pred_obj_heads, pred_obj_tails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6663, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_heads_loss = loss(data['sub_heads'], pred_sub_heads, data['mask'])\n",
    "sub_heads_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(36.7705, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_heads_loss = loss(data['sub_heads'], pred_sub_heads, data['mask'])\n",
    "sub_tails_loss = loss(data['sub_tails'], pred_sub_tails, data['mask'])\n",
    "obj_heads_loss = loss(data['obj_heads'], pred_obj_heads, data['mask'])\n",
    "obj_tails_loss = loss(data['obj_tails'], pred_obj_tails, data['mask'])\n",
    "total_loss = (sub_heads_loss + sub_tails_loss) + (obj_heads_loss + obj_tails_loss)\n",
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "total_loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step += 1\n",
    "loss_sum += total_loss.item()\n",
    "\n",
    "if global_step % con.period == 0:\n",
    "    cur_loss = loss_sum / self.config.period\n",
    "    elapsed = time.time() - start_time\n",
    "    logging(\"epoch: {:3d}, step: {:4d}, speed: {:5.2f}ms/b, train loss: {:5.3f}\".\n",
    "                 format(epoch, global_step, elapsed * 1000 / con.period, cur_loss))\n",
    "    loss_sum = 0\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 模型评价\n",
    "现在假设训练好了一个模型（或者模型训练了一个epoch），我们想看看模型现在的性能，那么就需要对模型进行评价\n",
    "\n",
    "**目录**\n",
    "* 预测头实体\n",
    "* 预测尾实体和关系\n",
    "* 计算评价指标\n",
    "* 定义函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result dir\n",
    "if not os.path.exists(con.result_dir):\n",
    "    os.mkdir(con.result_dir)\n",
    "\n",
    "path = os.path.join(con.result_dir, con.result_save_name)\n",
    "\n",
    "fw = open(path, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = ['subject', 'relation', 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tup(triple_list):\n",
    "    ret = []\n",
    "    for triple in triple_list:\n",
    "        ret.append(tuple(triple))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_prefetcher = data_loader.DataPreFetcher(dev_data_loader)\n",
    "data = test_data_prefetcher.next()\n",
    "id2rel = json.load(open(os.path.join(con.data_path, 'rel2id.json')))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': '/business/company/founders',\n",
       " '2': '/people/person/place_of_birth',\n",
       " '3': '/people/deceased_person/place_of_death',\n",
       " '4': '/business/company_shareholder/major_shareholder_of',\n",
       " '5': '/people/ethnicity/people',\n",
       " '6': '/location/neighborhood/neighborhood_of',\n",
       " '7': '/sports/sports_team/location',\n",
       " '9': '/business/company/industry',\n",
       " '10': '/business/company/place_founded',\n",
       " '11': '/location/administrative_division/country',\n",
       " '0': 'None',\n",
       " '12': '/sports/sports_team_location/teams',\n",
       " '13': '/people/person/nationality',\n",
       " '14': '/people/person/religion',\n",
       " '15': '/business/company/advisors',\n",
       " '16': '/people/person/ethnicity',\n",
       " '17': '/people/ethnicity/geographic_distribution',\n",
       " '8': '/business/person/company',\n",
       " '19': '/business/company/major_shareholders',\n",
       " '18': '/people/person/place_lived',\n",
       " '20': '/people/person/profession',\n",
       " '21': '/location/country/capital',\n",
       " '22': '/location/location/contains',\n",
       " '23': '/location/country/administrative_divisions',\n",
       " '24': '/people/person/children'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_num, predict_num, gold_num = 0, 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 预测头实体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    token_ids = data['token_ids']\n",
    "    tokens = data['tokens'][0]\n",
    "    mask = data['mask']\n",
    "    encoded_text = model.get_encoded_text(token_ids, mask)\n",
    "    pred_sub_heads, pred_sub_tails = model.get_subs(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 1])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sub_heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5773],\n",
       "         [0.5536],\n",
       "         [0.4459],\n",
       "         [0.4521],\n",
       "         [0.5084],\n",
       "         [0.5354],\n",
       "         [0.4858],\n",
       "         [0.5011],\n",
       "         [0.5999],\n",
       "         [0.5540],\n",
       "         [0.4826],\n",
       "         [0.4457],\n",
       "         [0.4614],\n",
       "         [0.4807],\n",
       "         [0.3697],\n",
       "         [0.4443],\n",
       "         [0.4665],\n",
       "         [0.5019],\n",
       "         [0.4635],\n",
       "         [0.4928],\n",
       "         [0.5673],\n",
       "         [0.5024],\n",
       "         [0.5179],\n",
       "         [0.5031],\n",
       "         [0.4897],\n",
       "         [0.4830],\n",
       "         [0.4964],\n",
       "         [0.5143],\n",
       "         [0.4795],\n",
       "         [0.5240]]], device='cuda:0')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sub_tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_bar=0.5\n",
    "t_bar=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_heads, sub_tails = np.where(pred_sub_heads.cpu()[0] > h_bar)[0], np.where(pred_sub_tails.cpu()[0] > t_bar)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  7, 11])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  4,  5,  7,  8,  9, 17, 20, 21, 22, 23, 27, 29])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = []\n",
    "for sub_head in sub_heads:\n",
    "    sub_tail = sub_tails[sub_tails >= sub_head]\n",
    "    if len(sub_tail) > 0:\n",
    "        sub_tail = sub_tail[0]\n",
    "        subject = tokens[sub_head: sub_tail]\n",
    "        subjects.append((subject, sub_head, sub_tail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([], 1, 1),\n",
       " ([',', 'north'], 2, 4),\n",
       " (['north'], 3, 4),\n",
       " ([], 7, 7),\n",
       " ([',', 'su', '##pp', '##lan', '##ted', 'a'], 11, 17)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 30, 768])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple_list = []\n",
    "# [subject_num, seq_len, bert_dim]\n",
    "repeated_encoded_text = encoded_text.repeat(len(subjects), 1, 1)\n",
    "repeated_encoded_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_head_mapping = torch.Tensor(len(subjects), 1, encoded_text.size(1)).zero_()\n",
    "sub_tail_mapping = torch.Tensor(len(subjects), 1, encoded_text.size(1)).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 30])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_tail_mapping.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([], 1, 1),\n",
       " ([',', 'north'], 2, 4),\n",
       " (['north'], 3, 4),\n",
       " ([], 7, 7),\n",
       " ([',', 'su', '##pp', '##lan', '##ted', 'a'], 11, 17)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject_idx, subject in enumerate(subjects):\n",
    "    sub_head_mapping[subject_idx][0][subject[1]] = 1\n",
    "    sub_tail_mapping[subject_idx][0][subject[2]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_tail_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tail_mapping = sub_tail_mapping.to(repeated_encoded_text)\n",
    "sub_head_mapping = sub_head_mapping.to(repeated_encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_tail_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 预测尾实体和关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_obj_heads, pred_obj_tails = model.get_objs_for_specific_sub(sub_head_mapping, sub_tail_mapping, repeated_encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 30, 25])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_obj_heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, ([], 1, 1)],\n",
       " [1, ([',', 'north'], 2, 4)],\n",
       " [2, (['north'], 3, 4)],\n",
       " [3, ([], 7, 7)],\n",
       " [4, ([',', 'su', '##pp', '##lan', '##ted', 'a'], 11, 17)]]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs = [[subject_idx, subject] for subject_idx, subject in enumerate(subjects)]\n",
    "subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], 1, 1)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject = subs[0][1]\n",
    "subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = subject[0]\n",
    "sub = ''.join([i.lstrip(\"##\") for i in sub])\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = ' '.join(sub.split('[unused1]'))\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 25])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_obj_heads[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_heads, obj_tails = np.where(pred_obj_heads.cpu()[subject_idx] > h_bar), np.where(pred_obj_tails.cpu()[subject_idx] > t_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309,)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_heads[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1],\n",
       " [0, 4],\n",
       " [0, 5],\n",
       " [0, 6],\n",
       " [0, 7],\n",
       " [0, 9],\n",
       " [0, 10],\n",
       " [0, 14],\n",
       " [0, 17],\n",
       " [0, 18],\n",
       " [0, 19],\n",
       " [1, 4],\n",
       " [1, 5],\n",
       " [1, 6],\n",
       " [1, 9],\n",
       " [1, 10],\n",
       " [1, 14],\n",
       " [1, 17],\n",
       " [1, 18],\n",
       " [1, 19],\n",
       " [1, 22],\n",
       " [2, 3],\n",
       " [2, 4],\n",
       " [2, 5],\n",
       " [2, 6],\n",
       " [2, 9],\n",
       " [2, 10],\n",
       " [2, 14],\n",
       " [2, 16],\n",
       " [2, 18],\n",
       " [2, 19],\n",
       " [2, 22],\n",
       " [3, 1],\n",
       " [3, 4],\n",
       " [3, 5],\n",
       " [3, 6],\n",
       " [3, 7],\n",
       " [3, 9],\n",
       " [3, 10],\n",
       " [3, 14],\n",
       " [3, 17],\n",
       " [3, 22],\n",
       " [4, 1],\n",
       " [4, 3],\n",
       " [4, 4],\n",
       " [4, 5],\n",
       " [4, 6],\n",
       " [4, 7],\n",
       " [4, 10],\n",
       " [4, 14],\n",
       " [4, 17],\n",
       " [4, 18],\n",
       " [4, 22],\n",
       " [5, 4],\n",
       " [5, 5],\n",
       " [5, 6],\n",
       " [5, 9],\n",
       " [5, 10],\n",
       " [5, 14],\n",
       " [5, 17],\n",
       " [5, 18],\n",
       " [5, 19],\n",
       " [5, 22],\n",
       " [6, 4],\n",
       " [6, 5],\n",
       " [6, 6],\n",
       " [6, 9],\n",
       " [6, 10],\n",
       " [6, 14],\n",
       " [6, 17],\n",
       " [6, 18],\n",
       " [6, 22],\n",
       " [7, 4],\n",
       " [7, 5],\n",
       " [7, 6],\n",
       " [7, 10],\n",
       " [7, 14],\n",
       " [7, 15],\n",
       " [7, 22],\n",
       " [8, 1],\n",
       " [8, 3],\n",
       " [8, 5],\n",
       " [8, 6],\n",
       " [8, 9],\n",
       " [8, 10],\n",
       " [8, 11],\n",
       " [8, 14],\n",
       " [8, 16],\n",
       " [8, 17],\n",
       " [8, 18],\n",
       " [8, 19],\n",
       " [8, 22],\n",
       " [9, 1],\n",
       " [9, 3],\n",
       " [9, 4],\n",
       " [9, 5],\n",
       " [9, 6],\n",
       " [9, 10],\n",
       " [9, 14],\n",
       " [9, 16],\n",
       " [9, 17],\n",
       " [9, 18],\n",
       " [9, 22],\n",
       " [10, 1],\n",
       " [10, 3],\n",
       " [10, 4],\n",
       " [10, 5],\n",
       " [10, 6],\n",
       " [10, 9],\n",
       " [10, 10],\n",
       " [10, 14],\n",
       " [10, 17],\n",
       " [10, 18],\n",
       " [10, 19],\n",
       " [10, 22],\n",
       " [11, 1],\n",
       " [11, 4],\n",
       " [11, 5],\n",
       " [11, 6],\n",
       " [11, 10],\n",
       " [11, 14],\n",
       " [11, 16],\n",
       " [11, 17],\n",
       " [11, 19],\n",
       " [11, 22],\n",
       " [12, 4],\n",
       " [12, 5],\n",
       " [12, 6],\n",
       " [12, 7],\n",
       " [12, 9],\n",
       " [12, 10],\n",
       " [12, 14],\n",
       " [12, 16],\n",
       " [12, 18],\n",
       " [12, 19],\n",
       " [13, 1],\n",
       " [13, 4],\n",
       " [13, 5],\n",
       " [13, 6],\n",
       " [13, 7],\n",
       " [13, 9],\n",
       " [13, 10],\n",
       " [13, 14],\n",
       " [13, 17],\n",
       " [13, 18],\n",
       " [13, 19],\n",
       " [14, 1],\n",
       " [14, 4],\n",
       " [14, 6],\n",
       " [14, 7],\n",
       " [14, 10],\n",
       " [14, 14],\n",
       " [14, 17],\n",
       " [14, 18],\n",
       " [14, 19],\n",
       " [15, 3],\n",
       " [15, 5],\n",
       " [15, 6],\n",
       " [15, 10],\n",
       " [15, 14],\n",
       " [15, 17],\n",
       " [15, 18],\n",
       " [15, 19],\n",
       " [15, 22],\n",
       " [16, 3],\n",
       " [16, 5],\n",
       " [16, 6],\n",
       " [16, 10],\n",
       " [16, 14],\n",
       " [16, 16],\n",
       " [16, 18],\n",
       " [16, 19],\n",
       " [16, 22],\n",
       " [17, 4],\n",
       " [17, 6],\n",
       " [17, 10],\n",
       " [17, 12],\n",
       " [17, 14],\n",
       " [17, 17],\n",
       " [17, 18],\n",
       " [18, 4],\n",
       " [18, 6],\n",
       " [18, 9],\n",
       " [18, 10],\n",
       " [18, 11],\n",
       " [18, 12],\n",
       " [18, 14],\n",
       " [18, 17],\n",
       " [18, 18],\n",
       " [19, 4],\n",
       " [19, 5],\n",
       " [19, 6],\n",
       " [19, 10],\n",
       " [19, 11],\n",
       " [19, 12],\n",
       " [19, 14],\n",
       " [19, 16],\n",
       " [19, 17],\n",
       " [19, 19],\n",
       " [19, 22],\n",
       " [20, 1],\n",
       " [20, 3],\n",
       " [20, 4],\n",
       " [20, 5],\n",
       " [20, 6],\n",
       " [20, 10],\n",
       " [20, 11],\n",
       " [20, 14],\n",
       " [20, 16],\n",
       " [20, 17],\n",
       " [20, 19],\n",
       " [20, 22],\n",
       " [21, 1],\n",
       " [21, 4],\n",
       " [21, 5],\n",
       " [21, 6],\n",
       " [21, 7],\n",
       " [21, 9],\n",
       " [21, 10],\n",
       " [21, 14],\n",
       " [21, 16],\n",
       " [21, 17],\n",
       " [21, 18],\n",
       " [21, 19],\n",
       " [21, 22],\n",
       " [22, 1],\n",
       " [22, 4],\n",
       " [22, 5],\n",
       " [22, 6],\n",
       " [22, 7],\n",
       " [22, 9],\n",
       " [22, 10],\n",
       " [22, 14],\n",
       " [22, 17],\n",
       " [22, 18],\n",
       " [22, 19],\n",
       " [22, 22],\n",
       " [23, 1],\n",
       " [23, 5],\n",
       " [23, 6],\n",
       " [23, 10],\n",
       " [23, 14],\n",
       " [23, 18],\n",
       " [23, 19],\n",
       " [23, 22],\n",
       " [24, 4],\n",
       " [24, 5],\n",
       " [24, 6],\n",
       " [24, 10],\n",
       " [24, 11],\n",
       " [24, 14],\n",
       " [24, 16],\n",
       " [24, 17],\n",
       " [24, 18],\n",
       " [24, 19],\n",
       " [24, 22],\n",
       " [25, 1],\n",
       " [25, 4],\n",
       " [25, 5],\n",
       " [25, 6],\n",
       " [25, 9],\n",
       " [25, 10],\n",
       " [25, 11],\n",
       " [25, 14],\n",
       " [25, 17],\n",
       " [25, 18],\n",
       " [25, 19],\n",
       " [25, 22],\n",
       " [26, 1],\n",
       " [26, 4],\n",
       " [26, 5],\n",
       " [26, 6],\n",
       " [26, 10],\n",
       " [26, 14],\n",
       " [26, 16],\n",
       " [26, 17],\n",
       " [26, 18],\n",
       " [26, 19],\n",
       " [26, 22],\n",
       " [27, 1],\n",
       " [27, 4],\n",
       " [27, 5],\n",
       " [27, 6],\n",
       " [27, 10],\n",
       " [27, 14],\n",
       " [27, 17],\n",
       " [27, 18],\n",
       " [27, 19],\n",
       " [27, 22],\n",
       " [28, 3],\n",
       " [28, 4],\n",
       " [28, 5],\n",
       " [28, 6],\n",
       " [28, 7],\n",
       " [28, 9],\n",
       " [28, 10],\n",
       " [28, 14],\n",
       " [28, 19],\n",
       " [28, 22],\n",
       " [29, 1],\n",
       " [29, 4],\n",
       " [29, 5],\n",
       " [29, 6],\n",
       " [29, 9],\n",
       " [29, 10],\n",
       " [29, 14],\n",
       " [29, 17],\n",
       " [29, 19],\n",
       " [29, 22]]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[obj_head, rel_head] for obj_head, rel_head in zip(*obj_heads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0],\n",
       " [0, 1],\n",
       " [0, 2],\n",
       " [0, 3],\n",
       " [0, 4],\n",
       " [0, 6],\n",
       " [0, 7],\n",
       " [0, 12],\n",
       " [0, 13],\n",
       " [0, 14],\n",
       " [0, 15],\n",
       " [0, 16],\n",
       " [0, 18],\n",
       " [0, 20],\n",
       " [0, 21],\n",
       " [0, 23],\n",
       " [0, 24],\n",
       " [1, 0],\n",
       " [1, 1],\n",
       " [1, 2],\n",
       " [1, 3],\n",
       " [1, 4],\n",
       " [1, 6],\n",
       " [1, 7],\n",
       " [1, 12],\n",
       " [1, 13],\n",
       " [1, 14],\n",
       " [1, 15],\n",
       " [1, 18],\n",
       " [1, 20],\n",
       " [1, 21],\n",
       " [1, 23],\n",
       " [1, 24],\n",
       " [2, 1],\n",
       " [2, 3],\n",
       " [2, 4],\n",
       " [2, 5],\n",
       " [2, 6],\n",
       " [2, 7],\n",
       " [2, 9],\n",
       " [2, 11],\n",
       " [2, 12],\n",
       " [2, 13],\n",
       " [2, 14],\n",
       " [2, 15],\n",
       " [2, 18],\n",
       " [2, 21],\n",
       " [2, 23],\n",
       " [2, 24],\n",
       " [3, 0],\n",
       " [3, 1],\n",
       " [3, 2],\n",
       " [3, 3],\n",
       " [3, 4],\n",
       " [3, 5],\n",
       " [3, 6],\n",
       " [3, 7],\n",
       " [3, 9],\n",
       " [3, 10],\n",
       " [3, 12],\n",
       " [3, 13],\n",
       " [3, 14],\n",
       " [3, 15],\n",
       " [3, 16],\n",
       " [3, 18],\n",
       " [3, 20],\n",
       " [3, 21],\n",
       " [3, 23],\n",
       " [3, 24],\n",
       " [4, 1],\n",
       " [4, 2],\n",
       " [4, 3],\n",
       " [4, 4],\n",
       " [4, 6],\n",
       " [4, 7],\n",
       " [4, 9],\n",
       " [4, 12],\n",
       " [4, 13],\n",
       " [4, 14],\n",
       " [4, 15],\n",
       " [4, 16],\n",
       " [4, 18],\n",
       " [4, 20],\n",
       " [4, 21],\n",
       " [4, 23],\n",
       " [4, 24],\n",
       " [5, 1],\n",
       " [5, 2],\n",
       " [5, 3],\n",
       " [5, 4],\n",
       " [5, 5],\n",
       " [5, 7],\n",
       " [5, 9],\n",
       " [5, 12],\n",
       " [5, 13],\n",
       " [5, 14],\n",
       " [5, 15],\n",
       " [5, 18],\n",
       " [5, 20],\n",
       " [5, 21],\n",
       " [5, 23],\n",
       " [5, 24],\n",
       " [6, 1],\n",
       " [6, 2],\n",
       " [6, 3],\n",
       " [6, 4],\n",
       " [6, 6],\n",
       " [6, 7],\n",
       " [6, 12],\n",
       " [6, 13],\n",
       " [6, 14],\n",
       " [6, 15],\n",
       " [6, 18],\n",
       " [6, 20],\n",
       " [6, 21],\n",
       " [6, 23],\n",
       " [6, 24],\n",
       " [7, 1],\n",
       " [7, 2],\n",
       " [7, 3],\n",
       " [7, 4],\n",
       " [7, 6],\n",
       " [7, 7],\n",
       " [7, 12],\n",
       " [7, 13],\n",
       " [7, 14],\n",
       " [7, 15],\n",
       " [7, 16],\n",
       " [7, 18],\n",
       " [7, 20],\n",
       " [7, 21],\n",
       " [7, 23],\n",
       " [8, 1],\n",
       " [8, 2],\n",
       " [8, 3],\n",
       " [8, 4],\n",
       " [8, 6],\n",
       " [8, 7],\n",
       " [8, 12],\n",
       " [8, 13],\n",
       " [8, 14],\n",
       " [8, 15],\n",
       " [8, 16],\n",
       " [8, 18],\n",
       " [8, 19],\n",
       " [8, 20],\n",
       " [8, 21],\n",
       " [8, 23],\n",
       " [9, 0],\n",
       " [9, 1],\n",
       " [9, 2],\n",
       " [9, 3],\n",
       " [9, 4],\n",
       " [9, 5],\n",
       " [9, 6],\n",
       " [9, 7],\n",
       " [9, 9],\n",
       " [9, 12],\n",
       " [9, 13],\n",
       " [9, 14],\n",
       " [9, 15],\n",
       " [9, 16],\n",
       " [9, 18],\n",
       " [9, 20],\n",
       " [9, 21],\n",
       " [9, 23],\n",
       " [9, 24],\n",
       " [10, 1],\n",
       " [10, 2],\n",
       " [10, 3],\n",
       " [10, 4],\n",
       " [10, 5],\n",
       " [10, 6],\n",
       " [10, 9],\n",
       " [10, 11],\n",
       " [10, 12],\n",
       " [10, 13],\n",
       " [10, 14],\n",
       " [10, 15],\n",
       " [10, 16],\n",
       " [10, 18],\n",
       " [10, 21],\n",
       " [10, 23],\n",
       " [10, 24],\n",
       " [11, 1],\n",
       " [11, 2],\n",
       " [11, 3],\n",
       " [11, 4],\n",
       " [11, 5],\n",
       " [11, 6],\n",
       " [11, 7],\n",
       " [11, 9],\n",
       " [11, 12],\n",
       " [11, 13],\n",
       " [11, 15],\n",
       " [11, 16],\n",
       " [11, 18],\n",
       " [11, 20],\n",
       " [11, 21],\n",
       " [11, 23],\n",
       " [11, 24],\n",
       " [12, 1],\n",
       " [12, 2],\n",
       " [12, 3],\n",
       " [12, 4],\n",
       " [12, 7],\n",
       " [12, 10],\n",
       " [12, 13],\n",
       " [12, 14],\n",
       " [12, 15],\n",
       " [12, 16],\n",
       " [12, 18],\n",
       " [12, 20],\n",
       " [12, 21],\n",
       " [12, 23],\n",
       " [12, 24],\n",
       " [13, 1],\n",
       " [13, 2],\n",
       " [13, 3],\n",
       " [13, 4],\n",
       " [13, 6],\n",
       " [13, 7],\n",
       " [13, 10],\n",
       " [13, 12],\n",
       " [13, 13],\n",
       " [13, 14],\n",
       " [13, 15],\n",
       " [13, 16],\n",
       " [13, 18],\n",
       " [13, 20],\n",
       " [13, 21],\n",
       " [13, 23],\n",
       " [13, 24],\n",
       " [14, 1],\n",
       " [14, 2],\n",
       " [14, 3],\n",
       " [14, 4],\n",
       " [14, 5],\n",
       " [14, 6],\n",
       " [14, 7],\n",
       " [14, 12],\n",
       " [14, 13],\n",
       " [14, 14],\n",
       " [14, 15],\n",
       " [14, 16],\n",
       " [14, 18],\n",
       " [14, 20],\n",
       " [14, 21],\n",
       " [14, 22],\n",
       " [14, 23],\n",
       " [15, 1],\n",
       " [15, 2],\n",
       " [15, 3],\n",
       " [15, 4],\n",
       " [15, 6],\n",
       " [15, 7],\n",
       " [15, 9],\n",
       " [15, 12],\n",
       " [15, 13],\n",
       " [15, 14],\n",
       " [15, 15],\n",
       " [15, 18],\n",
       " [15, 20],\n",
       " [15, 21],\n",
       " [15, 23],\n",
       " [15, 24],\n",
       " [16, 0],\n",
       " [16, 1],\n",
       " [16, 2],\n",
       " [16, 3],\n",
       " [16, 4],\n",
       " [16, 6],\n",
       " [16, 7],\n",
       " [16, 9],\n",
       " [16, 12],\n",
       " [16, 13],\n",
       " [16, 14],\n",
       " [16, 15],\n",
       " [16, 18],\n",
       " [16, 20],\n",
       " [16, 21],\n",
       " [16, 23],\n",
       " [16, 24],\n",
       " [17, 0],\n",
       " [17, 1],\n",
       " [17, 2],\n",
       " [17, 3],\n",
       " [17, 4],\n",
       " [17, 6],\n",
       " [17, 7],\n",
       " [17, 9],\n",
       " [17, 12],\n",
       " [17, 13],\n",
       " [17, 14],\n",
       " [17, 15],\n",
       " [17, 18],\n",
       " [17, 20],\n",
       " [17, 21],\n",
       " [17, 23],\n",
       " [18, 1],\n",
       " [18, 2],\n",
       " [18, 3],\n",
       " [18, 4],\n",
       " [18, 6],\n",
       " [18, 7],\n",
       " [18, 9],\n",
       " [18, 12],\n",
       " [18, 13],\n",
       " [18, 15],\n",
       " [18, 18],\n",
       " [18, 20],\n",
       " [18, 21],\n",
       " [18, 23],\n",
       " [18, 24],\n",
       " [19, 1],\n",
       " [19, 2],\n",
       " [19, 3],\n",
       " [19, 4],\n",
       " [19, 6],\n",
       " [19, 7],\n",
       " [19, 9],\n",
       " [19, 12],\n",
       " [19, 13],\n",
       " [19, 14],\n",
       " [19, 15],\n",
       " [19, 16],\n",
       " [19, 18],\n",
       " [19, 20],\n",
       " [19, 21],\n",
       " [19, 22],\n",
       " [19, 23],\n",
       " [20, 1],\n",
       " [20, 2],\n",
       " [20, 3],\n",
       " [20, 4],\n",
       " [20, 6],\n",
       " [20, 7],\n",
       " [20, 9],\n",
       " [20, 12],\n",
       " [20, 13],\n",
       " [20, 15],\n",
       " [20, 16],\n",
       " [20, 18],\n",
       " [20, 20],\n",
       " [20, 21],\n",
       " [20, 23],\n",
       " [20, 24],\n",
       " [21, 0],\n",
       " [21, 1],\n",
       " [21, 2],\n",
       " [21, 3],\n",
       " [21, 4],\n",
       " [21, 7],\n",
       " [21, 9],\n",
       " [21, 12],\n",
       " [21, 13],\n",
       " [21, 14],\n",
       " [21, 15],\n",
       " [21, 16],\n",
       " [21, 18],\n",
       " [21, 20],\n",
       " [21, 21],\n",
       " [21, 23],\n",
       " [21, 24],\n",
       " [22, 1],\n",
       " [22, 2],\n",
       " [22, 3],\n",
       " [22, 4],\n",
       " [22, 6],\n",
       " [22, 7],\n",
       " [22, 9],\n",
       " [22, 12],\n",
       " [22, 13],\n",
       " [22, 14],\n",
       " [22, 15],\n",
       " [22, 16],\n",
       " [22, 18],\n",
       " [22, 20],\n",
       " [22, 21],\n",
       " [22, 23],\n",
       " [22, 24],\n",
       " [23, 1],\n",
       " [23, 2],\n",
       " [23, 3],\n",
       " [23, 4],\n",
       " [23, 6],\n",
       " [23, 7],\n",
       " [23, 9],\n",
       " [23, 12],\n",
       " [23, 13],\n",
       " [23, 14],\n",
       " [23, 15],\n",
       " [23, 16],\n",
       " [23, 18],\n",
       " [23, 20],\n",
       " [23, 21],\n",
       " [23, 23],\n",
       " [24, 0],\n",
       " [24, 1],\n",
       " [24, 2],\n",
       " [24, 3],\n",
       " [24, 4],\n",
       " [24, 6],\n",
       " [24, 7],\n",
       " [24, 9],\n",
       " [24, 12],\n",
       " [24, 13],\n",
       " [24, 14],\n",
       " [24, 15],\n",
       " [24, 18],\n",
       " [24, 19],\n",
       " [24, 20],\n",
       " [24, 21],\n",
       " [24, 23],\n",
       " [25, 1],\n",
       " [25, 2],\n",
       " [25, 3],\n",
       " [25, 4],\n",
       " [25, 7],\n",
       " [25, 9],\n",
       " [25, 12],\n",
       " [25, 13],\n",
       " [25, 14],\n",
       " [25, 15],\n",
       " [25, 16],\n",
       " [25, 18],\n",
       " [25, 21],\n",
       " [25, 23],\n",
       " [25, 24],\n",
       " [26, 1],\n",
       " [26, 2],\n",
       " [26, 3],\n",
       " [26, 4],\n",
       " [26, 6],\n",
       " [26, 7],\n",
       " [26, 9],\n",
       " [26, 12],\n",
       " [26, 13],\n",
       " [26, 14],\n",
       " [26, 15],\n",
       " [26, 16],\n",
       " [26, 18],\n",
       " [26, 20],\n",
       " [26, 21],\n",
       " [26, 23],\n",
       " [27, 1],\n",
       " [27, 2],\n",
       " [27, 3],\n",
       " [27, 4],\n",
       " [27, 6],\n",
       " [27, 12],\n",
       " [27, 13],\n",
       " [27, 14],\n",
       " [27, 15],\n",
       " [27, 18],\n",
       " [27, 20],\n",
       " [27, 21],\n",
       " [27, 23],\n",
       " [27, 24],\n",
       " [28, 1],\n",
       " [28, 2],\n",
       " [28, 3],\n",
       " [28, 4],\n",
       " [28, 5],\n",
       " [28, 6],\n",
       " [28, 9],\n",
       " [28, 12],\n",
       " [28, 13],\n",
       " [28, 14],\n",
       " [28, 15],\n",
       " [28, 16],\n",
       " [28, 18],\n",
       " [28, 20],\n",
       " [28, 21],\n",
       " [28, 23],\n",
       " [28, 24],\n",
       " [29, 1],\n",
       " [29, 2],\n",
       " [29, 3],\n",
       " [29, 4],\n",
       " [29, 6],\n",
       " [29, 7],\n",
       " [29, 9],\n",
       " [29, 12],\n",
       " [29, 13],\n",
       " [29, 14],\n",
       " [29, 15],\n",
       " [29, 16],\n",
       " [29, 18],\n",
       " [29, 20],\n",
       " [29, 21],\n",
       " [29, 23],\n",
       " [29, 24]]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[obj_tail, rel_tail] for obj_tail, rel_tail in zip(*obj_tails)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj_head, rel_head in zip(*obj_heads):\n",
    "    for obj_tail, rel_tail in zip(*obj_tails):\n",
    "        if obj_head <= obj_tail and rel_head == rel_tail:\n",
    "            rel = id2rel[str(int(rel_head))]\n",
    "            obj = tokens[obj_head: obj_tail]\n",
    "            obj = ''.join([i.lstrip(\"##\") for i in obj])\n",
    "            obj = ' '.join(obj.split('[unused1]'))\n",
    "            triple_list.append((sub, rel, obj))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', '/business/company/founders', ''),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', 'inqueens'),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/sports/sports_team/location', ''),\n",
       " ('', '/business/company/industry', 'inqueens'),\n",
       " ('', '/business/company/place_founded', 'inqueens,'),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('',\n",
       "  '/business/company/major_shareholders',\n",
       "  'inqueens,northshoretowers,near'),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', 'queens'),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/business/company/industry', 'queens'),\n",
       " ('', '/business/company/place_founded', 'queens,'),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('', '/business/company/major_shareholders', 'queens,northshoretowers,near'),\n",
       " ('',\n",
       "  '/location/location/contains',\n",
       "  'queens,northshoretowers,nearthenassauborder,supp'),\n",
       " ('', '/people/deceased_person/place_of_death', ''),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', ''),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/business/company/industry', ''),\n",
       " ('', '/business/company/place_founded', ','),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/ethnicity', ','),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('', '/business/company/major_shareholders', ',northshoretowers,near'),\n",
       " ('',\n",
       "  '/location/location/contains',\n",
       "  ',northshoretowers,nearthenassauborder,supp'),\n",
       " ('', '/business/company/founders', ''),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', ''),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/sports/sports_team/location', ''),\n",
       " ('', '/business/company/industry', ''),\n",
       " ('', '/business/company/place_founded', ''),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('',\n",
       "  '/location/location/contains',\n",
       "  'northshoretowers,nearthenassauborder,supp'),\n",
       " ('', '/business/company/founders', ''),\n",
       " ('', '/people/deceased_person/place_of_death', ''),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', 'shore'),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/sports/sports_team/location', ''),\n",
       " ('', '/business/company/place_founded', 'shoretowers,nearthenassauborder,'),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('', '/location/location/contains', 'shoretowers,nearthenassauborder,supp'),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', ''),\n",
       " ('', '/location/neighborhood/neighborhood_of', 'towers'),\n",
       " ('', '/business/company/industry', ''),\n",
       " ('', '/business/company/place_founded', 'towers,nearthenassauborder,'),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('', '/business/company/major_shareholders', 'towers,near'),\n",
       " ('', '/location/location/contains', 'towers,nearthenassauborder,supp'),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', ',nearthe'),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/business/company/industry', ',nearthe'),\n",
       " ('', '/business/company/place_founded', ',nearthenassauborder,'),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('', '/location/location/contains', ',nearthenassauborder,supp'),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', 'nearthe'),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/business/company/place_founded', 'nearthenassauborder,'),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/business/company/advisors', ''),\n",
       " ('', '/location/location/contains', 'nearthenassauborder,supp'),\n",
       " ('', '/business/company/founders', ''),\n",
       " ('', '/people/deceased_person/place_of_death', ''),\n",
       " ('', '/people/ethnicity/people', 'the'),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/business/company/industry', 'the'),\n",
       " ('', '/business/company/place_founded', 'thenassauborder,'),\n",
       " ('', '/location/administrative_division/country', 'thenassau'),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/ethnicity', ''),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('', '/business/company/major_shareholders', ''),\n",
       " ('', '/location/location/contains', 'thenassauborder,supp'),\n",
       " ('', '/business/company/founders', ''),\n",
       " ('', '/people/deceased_person/place_of_death', ''),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', ''),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/business/company/place_founded', 'nassauborder,'),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/ethnicity', ''),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('', '/location/location/contains', 'nassauborder,supp'),\n",
       " ('', '/business/company/founders', ''),\n",
       " ('', '/people/deceased_person/place_of_death', ''),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', ''),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/business/company/industry', ''),\n",
       " ('', '/business/company/place_founded', 'border,'),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('',\n",
       "  '/business/company/major_shareholders',\n",
       "  'border,supplantedagolfcourse,andhousingreplaceda'),\n",
       " ('', '/location/location/contains', 'border,supp'),\n",
       " ('', '/business/company/founders', ''),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', ''),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/business/company/place_founded', ','),\n",
       " ('', '/people/person/religion', ','),\n",
       " ('', '/people/person/ethnicity', ''),\n",
       " ('',\n",
       "  '/business/company/major_shareholders',\n",
       "  ',supplantedagolfcourse,andhousingreplaceda'),\n",
       " ('', '/location/location/contains', ',supp'),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', 'supp'),\n",
       " ('', '/location/neighborhood/neighborhood_of', 'su'),\n",
       " ('', '/sports/sports_team/location', ''),\n",
       " ('', '/business/company/industry', 'supplan'),\n",
       " ('', '/business/company/place_founded', ''),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/ethnicity', ''),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('',\n",
       "  '/business/company/major_shareholders',\n",
       "  'supplantedagolfcourse,andhousingreplaceda'),\n",
       " ('', '/business/company/founders', ''),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', 'pp'),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/sports/sports_team/location', ''),\n",
       " ('', '/business/company/industry', 'pplan'),\n",
       " ('', '/business/company/place_founded', ''),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('',\n",
       "  '/business/company/major_shareholders',\n",
       "  'pplantedagolfcourse,andhousingreplaceda'),\n",
       " ('', '/business/company/founders', ''),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/sports/sports_team/location', ''),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('',\n",
       "  '/business/company/major_shareholders',\n",
       "  'lantedagolfcourse,andhousingreplaceda'),\n",
       " ('', '/people/deceased_person/place_of_death', ''),\n",
       " ('',\n",
       "  '/people/ethnicity/people',\n",
       "  'tedagolfcourse,andhousingreplacedagravelquarryindouglas'),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('',\n",
       "  '/business/company/major_shareholders',\n",
       "  'tedagolfcourse,andhousingreplaceda'),\n",
       " ('', '/location/location/contains', 'tedagolfcourse'),\n",
       " ('', '/people/deceased_person/place_of_death', ''),\n",
       " ('',\n",
       "  '/people/ethnicity/people',\n",
       "  'agolfcourse,andhousingreplacedagravelquarryindouglas'),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/ethnicity', 'agolfcourse'),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('',\n",
       "  '/business/company/major_shareholders',\n",
       "  'agolfcourse,andhousingreplaceda'),\n",
       " ('', '/location/location/contains', 'agolfcourse'),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/sports/sports_team_location/teams', ''),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/business/company/industry', ''),\n",
       " ('', '/sports/sports_team_location/teams', ''),\n",
       " ('', '/people/person/religion', 'course'),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', ',andhousingreplacedagravelquarryindouglas'),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/sports/sports_team_location/teams', ''),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/ethnicity', ''),\n",
       " ('', '/business/company/major_shareholders', ',andhousingreplaceda'),\n",
       " ('', '/location/location/contains', ''),\n",
       " ('', '/business/company/founders', ''),\n",
       " ('', '/people/deceased_person/place_of_death', ''),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', 'andhousingreplacedagravelquarryindouglas'),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/people/person/religion', 'and'),\n",
       " ('', '/people/person/ethnicity', ''),\n",
       " ('', '/business/company/major_shareholders', 'andhousingreplaceda'),\n",
       " ('', '/business/company/founders', ''),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', 'housingreplacedagravelquarryindouglas'),\n",
       " ('', '/location/neighborhood/neighborhood_of', 'housing'),\n",
       " ('', '/sports/sports_team/location', ''),\n",
       " ('', '/business/company/industry', ''),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/ethnicity', ''),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('', '/business/company/major_shareholders', 'housingreplaceda'),\n",
       " ('', '/business/company/founders', ''),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', 'replacedagravelquarryindouglas'),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/sports/sports_team/location', ''),\n",
       " ('', '/business/company/industry', ''),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('', '/business/company/major_shareholders', 'replaceda'),\n",
       " ('', '/business/company/founders', ''),\n",
       " ('', '/people/ethnicity/people', 'agravelquarryindouglas'),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('', '/business/company/major_shareholders', 'a'),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', 'gravelquarryindouglas'),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/ethnicity', 'gravel'),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('', '/business/company/major_shareholders', ''),\n",
       " ('', '/business/company/founders', ''),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', 'quarryindouglas'),\n",
       " ('', '/location/neighborhood/neighborhood_of', 'quarry'),\n",
       " ('', '/business/company/industry', ''),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('', '/business/company/founders', ''),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', 'indouglas'),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/ethnicity', ''),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('', '/business/company/founders', ''),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', 'douglas'),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('', '/people/deceased_person/place_of_death', ''),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/ethnicity/people', ''),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/sports/sports_team/location', 'ton'),\n",
       " ('', '/business/company/industry', ''),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/business/company/founders', ''),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/business/company/industry', ''),\n",
       " ('', '/people/person/religion', '')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4.5.3 计算评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', '/business/company/industry', 'queens'),\n",
       " ('', '/people/ethnicity/people', 'queens'),\n",
       " ('', '/location/neighborhood/neighborhood_of', 'quarry'),\n",
       " ('', '/business/company/place_founded', 'towers,nearthenassauborder,'),\n",
       " ('', '/people/person/ethnicity', 'gravel'),\n",
       " ('', '/location/neighborhood/neighborhood_of', 'housing'),\n",
       " ('', '/business/company/industry', 'the'),\n",
       " ('',\n",
       "  '/location/location/contains',\n",
       "  'northshoretowers,nearthenassauborder,supp'),\n",
       " ('', '/sports/sports_team/location', ''),\n",
       " ('', '/people/ethnicity/people', 'housingreplacedagravelquarryindouglas'),\n",
       " ('', '/business/company/place_founded', 'nassauborder,'),\n",
       " ('', '/sports/sports_team/location', 'ton'),\n",
       " ('', '/business/company_shareholder/major_shareholder_of', ''),\n",
       " ('', '/people/person/religion', 'course'),\n",
       " ('', '/people/ethnicity/people', 'quarryindouglas'),\n",
       " ('', '/people/ethnicity/people', ',nearthe'),\n",
       " ('', '/location/location/contains', ',nearthenassauborder,supp'),\n",
       " ('', '/location/location/contains', 'thenassauborder,supp'),\n",
       " ('', '/people/person/place_lived', ''),\n",
       " ('', '/business/company/place_founded', 'border,'),\n",
       " ('', '/business/company/place_founded', 'queens,'),\n",
       " ('', '/business/company/major_shareholders', ',andhousingreplaceda'),\n",
       " ('', '/location/location/contains', 'agolfcourse'),\n",
       " ('', '/business/company/industry', 'supplan'),\n",
       " ('', '/location/neighborhood/neighborhood_of', 'su'),\n",
       " ('', '/business/company/major_shareholders', 'queens,northshoretowers,near'),\n",
       " ('',\n",
       "  '/business/company/major_shareholders',\n",
       "  'agolfcourse,andhousingreplaceda'),\n",
       " ('',\n",
       "  '/business/company/major_shareholders',\n",
       "  'tedagolfcourse,andhousingreplaceda'),\n",
       " ('', '/business/company/place_founded', 'nearthenassauborder,'),\n",
       " ('', '/people/person/religion', 'and'),\n",
       " ('', '/business/company/major_shareholders', 'towers,near'),\n",
       " ('', '/people/ethnicity/people', 'andhousingreplacedagravelquarryindouglas'),\n",
       " ('', '/location/neighborhood/neighborhood_of', 'towers'),\n",
       " ('', '/location/location/contains', 'border,supp'),\n",
       " ('', '/people/person/religion', ','),\n",
       " ('', '/people/person/ethnicity', ','),\n",
       " ('',\n",
       "  '/business/company/major_shareholders',\n",
       "  'supplantedagolfcourse,andhousingreplaceda'),\n",
       " ('', '/people/person/ethnicity', 'agolfcourse'),\n",
       " ('', '/people/ethnicity/people', 'shore'),\n",
       " ('', '/business/company/industry', ',nearthe'),\n",
       " ('', '/business/company/major_shareholders', 'a'),\n",
       " ('', '/location/location/contains', 'shoretowers,nearthenassauborder,supp'),\n",
       " ('', '/business/company/major_shareholders', 'housingreplaceda'),\n",
       " ('', '/people/ethnicity/people', 'nearthe'),\n",
       " ('',\n",
       "  '/business/company/major_shareholders',\n",
       "  'pplantedagolfcourse,andhousingreplaceda'),\n",
       " ('',\n",
       "  '/business/company/major_shareholders',\n",
       "  'lantedagolfcourse,andhousingreplaceda'),\n",
       " ('',\n",
       "  '/business/company/major_shareholders',\n",
       "  'border,supplantedagolfcourse,andhousingreplaceda'),\n",
       " ('', '/location/neighborhood/neighborhood_of', ''),\n",
       " ('', '/people/ethnicity/people', 'agravelquarryindouglas'),\n",
       " ('', '/people/ethnicity/people', 'pp'),\n",
       " ('', '/people/ethnicity/people', ',andhousingreplacedagravelquarryindouglas'),\n",
       " ('', '/business/company/major_shareholders', 'andhousingreplaceda'),\n",
       " ('', '/business/company/major_shareholders', 'replaceda'),\n",
       " ('', '/business/company/industry', 'pplan'),\n",
       " ('', '/business/company/place_founded', ','),\n",
       " ('',\n",
       "  '/people/ethnicity/people',\n",
       "  'tedagolfcourse,andhousingreplacedagravelquarryindouglas'),\n",
       " ('', '/people/ethnicity/people', 'supp'),\n",
       " ('', '/location/location/contains', 'nearthenassauborder,supp'),\n",
       " ('',\n",
       "  '/location/location/contains',\n",
       "  ',northshoretowers,nearthenassauborder,supp'),\n",
       " ('', '/business/company/place_founded', 'shoretowers,nearthenassauborder,'),\n",
       " ('', '/location/location/contains', 'nassauborder,supp'),\n",
       " ('',\n",
       "  '/business/company/major_shareholders',\n",
       "  ',supplantedagolfcourse,andhousingreplaceda'),\n",
       " ('',\n",
       "  '/business/company/major_shareholders',\n",
       "  'inqueens,northshoretowers,near'),\n",
       " ('',\n",
       "  '/location/location/contains',\n",
       "  'queens,northshoretowers,nearthenassauborder,supp'),\n",
       " ('', '/business/company/advisors', ''),\n",
       " ('', '/business/company/place_founded', 'thenassauborder,'),\n",
       " ('', '/business/company/place_founded', 'inqueens,'),\n",
       " ('', '/location/location/contains', ''),\n",
       " ('', '/people/deceased_person/place_of_death', ''),\n",
       " ('', '/people/ethnicity/people', 'indouglas'),\n",
       " ('', '/sports/sports_team_location/teams', ''),\n",
       " ('', '/people/ethnicity/people', 'gravelquarryindouglas'),\n",
       " ('', '/location/administrative_division/country', 'thenassau'),\n",
       " ('', '/people/ethnicity/people', 'replacedagravelquarryindouglas'),\n",
       " ('', '/business/company/industry', ''),\n",
       " ('', '/people/ethnicity/people', 'the'),\n",
       " ('', '/people/ethnicity/people', ''),\n",
       " ('', '/location/location/contains', 'tedagolfcourse'),\n",
       " ('', '/people/ethnicity/people', 'douglas'),\n",
       " ('', '/business/company/place_founded', ''),\n",
       " ('', '/location/location/contains', ',supp'),\n",
       " ('', '/location/location/contains', 'towers,nearthenassauborder,supp'),\n",
       " ('', '/business/company/founders', ''),\n",
       " ('', '/people/person/religion', ''),\n",
       " ('', '/people/person/ethnicity', ''),\n",
       " ('',\n",
       "  '/people/ethnicity/people',\n",
       "  'agolfcourse,andhousingreplacedagravelquarryindouglas'),\n",
       " ('', '/business/company/place_founded', ',nearthenassauborder,'),\n",
       " ('', '/business/company/industry', 'inqueens'),\n",
       " ('', '/business/company/major_shareholders', ''),\n",
       " ('', '/people/ethnicity/people', 'inqueens'),\n",
       " ('', '/business/company/major_shareholders', ',northshoretowers,near')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple_set = set()\n",
    "for s, r, o in triple_list:\n",
    "    triple_set.add((s, r, o))\n",
    "pred_list = list(triple_set)\n",
    "pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Douglaston', '/location/neighborhood/neighborhood_of', 'Queens'],\n",
       "  ['Queens', '/location/location/contains', 'Douglaston']]]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['triples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_triples = set(pred_list)\n",
    "gold_triples = set(to_tup(data['triples'][0]))\n",
    "\n",
    "correct_num += len(pred_triples & gold_triples)\n",
    "predict_num += len(pred_triples)\n",
    "gold_num += len(gold_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = correct_num / (predict_num + 1e-10)\n",
    "recall = correct_num / (gold_num + 1e-10)\n",
    "f1_score = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.4  定义测试函数，方便调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_data_loader, model, output=False, h_bar=0.5, t_bar=0.5):\n",
    "\n",
    "    if output:\n",
    "        # check the result dir\n",
    "        if not os.path.exists(con.result_dir):\n",
    "            os.mkdir(con.result_dir)\n",
    "\n",
    "        path = os.path.join(con.result_dir, con.result_save_name)\n",
    "\n",
    "        fw = open(path, 'w')\n",
    "\n",
    "    orders = ['subject', 'relation', 'object']\n",
    "\n",
    "    def to_tup(triple_list):\n",
    "        ret = []\n",
    "        for triple in triple_list:\n",
    "            ret.append(tuple(triple))\n",
    "        return ret\n",
    "\n",
    "    test_data_prefetcher = data_loader.DataPreFetcher(test_data_loader)\n",
    "    data = test_data_prefetcher.next()\n",
    "    id2rel = json.load(open(os.path.join(con.data_path, 'rel2id.json')))[0]\n",
    "    correct_num, predict_num, gold_num = 0, 0, 0\n",
    "\n",
    "    while data is not None:\n",
    "        with torch.no_grad():\n",
    "            token_ids = data['token_ids']\n",
    "            tokens = data['tokens'][0]\n",
    "            mask = data['mask']\n",
    "            encoded_text = model.get_encoded_text(token_ids, mask)\n",
    "            pred_sub_heads, pred_sub_tails = model.get_subs(encoded_text)\n",
    "            sub_heads, sub_tails = np.where(pred_sub_heads.cpu()[0] > h_bar)[0], np.where(pred_sub_tails.cpu()[0] > t_bar)[0]\n",
    "            subjects = []\n",
    "            for sub_head in sub_heads:\n",
    "                sub_tail = sub_tails[sub_tails >= sub_head]\n",
    "                if len(sub_tail) > 0:\n",
    "                    sub_tail = sub_tail[0]\n",
    "                    subject = tokens[sub_head: sub_tail]\n",
    "                    subjects.append((subject, sub_head, sub_tail))\n",
    "            if subjects:\n",
    "                triple_list = []\n",
    "                # [subject_num, seq_len, bert_dim]\n",
    "                repeated_encoded_text = encoded_text.repeat(len(subjects), 1, 1)\n",
    "                # [subject_num, 1, seq_len]\n",
    "                sub_head_mapping = torch.Tensor(len(subjects), 1, encoded_text.size(1)).zero_()\n",
    "                sub_tail_mapping = torch.Tensor(len(subjects), 1, encoded_text.size(1)).zero_()\n",
    "                for subject_idx, subject in enumerate(subjects):\n",
    "                    sub_head_mapping[subject_idx][0][subject[1]] = 1\n",
    "                    sub_tail_mapping[subject_idx][0][subject[2]] = 1\n",
    "                sub_tail_mapping = sub_tail_mapping.to(repeated_encoded_text)\n",
    "                sub_head_mapping = sub_head_mapping.to(repeated_encoded_text)\n",
    "                pred_obj_heads, pred_obj_tails = model.get_objs_for_specific_sub(sub_head_mapping, sub_tail_mapping, repeated_encoded_text)\n",
    "                for subject_idx, subject in enumerate(subjects):\n",
    "                    sub = subject[0]\n",
    "                    sub = ''.join([i.lstrip(\"##\") for i in sub])\n",
    "                    sub = ' '.join(sub.split('[unused1]'))\n",
    "                    obj_heads, obj_tails = np.where(pred_obj_heads.cpu()[subject_idx] > h_bar), np.where(pred_obj_tails.cpu()[subject_idx] > t_bar)\n",
    "                    for obj_head, rel_head in zip(*obj_heads):\n",
    "                        for obj_tail, rel_tail in zip(*obj_tails):\n",
    "                            if obj_head <= obj_tail and rel_head == rel_tail:\n",
    "                                rel = id2rel[str(int(rel_head))]\n",
    "                                obj = tokens[obj_head: obj_tail]\n",
    "                                obj = ''.join([i.lstrip(\"##\") for i in obj])\n",
    "                                obj = ' '.join(obj.split('[unused1]'))\n",
    "                                triple_list.append((sub, rel, obj))\n",
    "                                break\n",
    "                triple_set = set()\n",
    "                for s, r, o in triple_list:\n",
    "                    triple_set.add((s, r, o))\n",
    "                pred_list = list(triple_set)\n",
    "            else:\n",
    "                pred_list = []\n",
    "            pred_triples = set(pred_list)\n",
    "            gold_triples = set(to_tup(data['triples'][0]))\n",
    "\n",
    "            correct_num += len(pred_triples & gold_triples)\n",
    "            predict_num += len(pred_triples)\n",
    "            gold_num += len(gold_triples)\n",
    "\n",
    "            if output:\n",
    "                result = json.dumps({\n",
    "                    # 'text': ' '.join(tokens),\n",
    "                    'triple_list_gold': [\n",
    "                        dict(zip(orders, triple)) for triple in gold_triples\n",
    "                    ],\n",
    "                    'triple_list_pred': [\n",
    "                        dict(zip(orders, triple)) for triple in pred_triples\n",
    "                    ],\n",
    "                    'new': [\n",
    "                        dict(zip(orders, triple)) for triple in pred_triples - gold_triples\n",
    "                    ],\n",
    "                    'lack': [\n",
    "                        dict(zip(orders, triple)) for triple in gold_triples - pred_triples\n",
    "                    ]\n",
    "                }, ensure_ascii=False)\n",
    "                fw.write(result + '\\n')\n",
    "\n",
    "            data = test_data_prefetcher.next()\n",
    "\n",
    "    print(\"correct_num: {:3d}, predict_num: {:3d}, gold_num: {:3d}\".format(correct_num, predict_num, gold_num))\n",
    "\n",
    "    precision = correct_num / (predict_num + 1e-10)\n",
    "    recall = correct_num / (gold_num + 1e-10)\n",
    "    f1_score = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 代码梳理及细节回顾(在VScode中演示)\n",
    "\n",
    "　　在VScode环境中的训练文件里再回顾训练流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 作业\n",
    "  \n",
    "`【思考题】`思考这篇文章的模型有什么不足，有什么可以改进的地方。\n",
    "\n",
    "`【代码实践】`复现该文章代码的模型（CASREL）部分。\n",
    "\n",
    "`【画图】`不看文章原图，按照自己的理解画出模型的结构图。\n",
    "\n",
    "`【总结】`对这篇文章进行回顾，思考并学习文章写作总体结构，实验设计等内容。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTH_1.6",
   "language": "python",
   "name": "pyth_1.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
